<!DOCTYPE HTML>
<!--
	Helios by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Continuous and Open-Set Learning Workshop @ CVPR 2017</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body class="homepage">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">

					<!-- Inner -->
						<div class="inner">
							<header>
                                <h1><a href="index.html" id="logo">Continuous and<br>Open-Set Learning<br> Workshop @ CVPR 2017</a></h1>
								<hr />
                                <p>Workshop at the Computer Vision and Pattern Recognition Conference<br>
                                July 26th, 2017<br>
                                Deadline for Abstracts: May 31st 2017<br>
                                </p>
							</header>
							<footer>
								<a href="#speakers" class="button circled scrolly">Start</a>
							</footer>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="#cfp">Call for Abstracts</a></li>
                <!--                                <li><a href="#dates">Important Dates</a></li>-->
								<li><a href="#organizers">Organizers</a></li>
                    			<li><a href="#papers">Accepted Papers</a></li>
                                <li><a href="#program">Program</a></li> 
								<li><a href="http://cvpr2017.thecvf.com" target="new">CVPR 2017</a></li>
							</ul>
						</nav>

				</div>


            <!--
                <div class="wrapper style2" id="dates">
					<article class="container special">
					<header>
                        <h2><a href="#">Important Dates</a></h2>
				    </header>
                    <ul>
                        <li>Deadline for extended abstract submissions: <strong>15th of April, 2016</strong></li>
                        <li>Acceptance notification: 30th of April, 2016</li>
                        <li>Workshop: 19th of June, 2016</li>
                    </ul>
                    <p>
                    Extended abstracts need to be submitted by email to <a href="mailto:carlhenrik.ek@bristol.ac.uk">Carl Henrik Ek</a>.<br>
                    <strong>We will not use the PaperCept system.</strong>
                    </p>

                    <p>
                    Formatting guidelines and submission rules:
                    <ul style="margin-left:15%">
                        <li>1. Maximum of four pages including references</li>
                        <li>2. <a href="http://iv2016.org/authors/authors-2/" target="new">IEEE format</a></li>
                        <li>3. The extended abstracts will not be published in the proceedings. However they will be
                            published on this website if the authors agree.</li>
                        <li>4. The submitted abstract should present recent results, but not necessarily novel ones.</li>
                        <li>5. All accepted abstracts will be presented as posters at the workshop. We will select a single
                            abstract which will be additionally given the opportunity for an oral presentation.</li>
                        <li>6. All abstracts will be reviewed by the organizers.</li>
                    </ul>
                    </p>
                    </article>
				</div>
            -->
				<div class="wrapper style2" id="speakers">
					<article class="container special">
						<header>
							<h2><a href="#">Keynote Speakers</a></h2>
						</header>
                        <ul style="margin-left:15%;">
                            <li>
                            <a href="http://homepages.inf.ed.ac.uk/vferrari/" target="new">
                                <img src="images/vitto.png" style="max-width:20%;min-width:20%;vertical-align:top;margin-right:5%;float:left;" />
                                <b>Vittorio Ferrari (University of Edinburgh, Google Zurich)</b>
                            </a><br>
                            Vitto is a Professor at the School of Informatics of the University of Edinburgh leading the CALVIN research group and currently also
                            building a research group at Google Research Zurich. 
                            He received his PhD from ETH Zurich in 2004 and was a post-doctoral researcher at INRIA Grenoble in 2006-2007 and at the University of Oxford in 2007-2008. Between 2008 and 2012 he was Assistant Professor at ETH Zurich, funded by a Swiss National Science Foundation Professorship grant. He received the prestigious ERC Starting Grant, and the best paper award from the European Conference in Computer Vision, both in 2012. 
                            He is an Associate Editor of IEEE Pattern Analysis and Machine Intelligence and will be a Program Chair at ECCV 2018. 
							</li>
                            <div style="clear:left;">
                            <li style="margin-top: 5%;">
                                <a href="http://www.cs.utoronto.ca/~fidler/" target="new">
                                <img src="images/fidler.jpg" style="max-width:20%;min-width:20%;vertical-align:top;margin-right:5%;float:left;" />
                                <b>Sanja Fidler (University of Toronto, Canada)</b>
                                </a><br>
                                Sanja Fidler is an assistant professor at the University of Toronto. Previously, she has been a research
                                assitant professor at the Toyota Technological Institute at Chicago as as a PostDoc in Sven Dickinson's group in Toronto.
                                Her research interests are in 2D and 3D object detection as well as in the interplay between language and vision: generating sentential descriptions about complex scenes, as well as using textual descriptions for better scene parsing. 
                            </li>

                            <div style="clear:left;">
                            <li style="margin-top: 10%;">
                            <a href="https://people.eecs.berkeley.edu/~trevor/" target="new">
                                <img src="images/darrell.jpg" style="max-width:20%;min-width:20%;vertical-align:top;margin-right:5%;float:left;" />
                                <b>Trevor Darrell (University of California, Berkeley)</b>
                            </a><br>
							Trevor is on the faculty of the CS Division of the EECS Department at UC Berkeley. He leads Berkeley’s DeepDrive Industrial Consortia, is co-Director of the Berkeley Artificial Intelligence Research (BAIR) lab, and is Faculty Director of PATH at UC Berkeley. 
                            Trevor’s group develops algorithms for large-scale perceptual learning, including object and activity recognition and detection.
                            Trevor was on the faculty of the MIT EECS department from 1999-2008, where he directed the Vision Interface Group. 
                            He was a member of the research staff at Interval Research Corporation from 1996-1999, and received the S.M., and PhD. degrees from MIT in 1992 and 1996, respectively.

                            </li>
                            <div style="clear:left;">
                            <li>
                            <i>Abstract</i>:
                           Learning of layered or "deep" representations has provided significant advances in computer vision in recent years, but has traditionally been limited to fully supervised settings with very large amounts of training data.  New results in adversarial adaptive representation learning show how such methods can also excel when learning in sparse/weakly labeled settings across modalities and domains. I'll also describe recent results on learning representations in a reinforcement learning setting, incorporating self-supervision losses and curiosity driven exploration into traditional reward-based optimization.   As time permits, I'll present recent long-term recurrent network models that learn cross-modal description and explanation, visuomotor robotic policies that adapt to new domains, and deep autonomous driving policies that can be learned from heterogeneous large-scale dashcam video datasets. 
                            </li>
                            <div style="clear:left;">
                            <li style="margin-top: 5%;">
                                <a href="https://scholar.google.com/citations?user=Q0YEc-QAAAAJ&hl=en" target="new">
                                <img src="images/hadsell.jpg" style="max-width:20%;min-width:20%;vertical-align:top;margin-right:5%;float:left;" />
                                <b>Raia Hadsell (Google DeepMind, London)</b>
                                </a><br>
                                Raia Hadsell, a senior research scientist at Google DeepMind, has worked on deep learning and robotics problems for over 10 years. Her thesis on Vision for Mobile Robots won the Best Dissertation award from New York University, and was followed by a post-doc at Carnegie Mellon’s Robotics Institute. Raia then worked as a senior scientist and tech manager at SRI International. Raia joined DeepMind in 2014, where she leads a research team studying robot navigation and lifelong learning.
                                </li>
                            <div style="clear:left;">
                            <li>
                            <i>Abstract</i>:
                            Continual learning is an important problem for reinforcement learning, because RL agents are trained sequentially, in interactive environments, and are especially vulnerable to the phenomena of catastrophic forgetting and catastrophic interference. Successful methods for continual learning have broad potential, because they could enable agents to learn multiple skills, potentially enabling complex behaviors. I will describe continual learning methods for such
                            agents.
                            </li>

                        </ul>
					</article>
				</div>

<
				<div class="wrapper style2" id="program">
					<article class="container special">
						<header>
							<h2><a href="#">Program</a></h2>
						</header>
 
              <table>
               <tr>
                   <td class="agenda-time">
                    08:30am - 08:40am
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-chevron-left fa-fw"></i> Welcome and Introduction<br/>
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                      08:40am - 09:20am
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-microphone fa-fw"></i> Raia Hadsell "Deep Reinforcement Learning in Sequential Environments"
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                      09:20am - 10:00am
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-microphone fa-fw"></i> Trevor Darrell "Adaptive Representation Learning for Perception, Action, and Explanation"
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                    10:00am - 10:30am
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-coffee fa-fw"></i> Refreshment Break with Poster Session (Poster)
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                      10:30am - 11:10am
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-microphone fa-fw"></i> Research Teaser Talks Session 1 (Teaser-S1)
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                      11:10am - 11:50am
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-microphone fa-fw"></i> Vittorio Ferrari "Towards continuous learning of object class detectors"
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                    12:00am - 02:20pm
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-cutlery fa-fw"></i> Lunch Break
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                      02:20pm - 03:00pm
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-microphone fa-fw"></i> Research Teaser Talks Session 2 (Teaser-S2)
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                      03:00pm - 03:40pm
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-microphone  fa-fw"></i> Invited talk by Sanja Fidler
                    </div>
                  </td>
                </tr>
 
                <tr>
                </tr>
                <tr>
                  <td class="agenda-time">
                    03:40pm - 04:15pm
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-coffee fa-fw"></i> Refreshment Break with Poster Session (Poster)
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                    04:15pm - 05:00pm
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-group fa-fw"></i> Panel Discussion
                    </div>
                  </td>
                </tr>
                <tr>
                  <td class="agenda-time">
                    05:00pm
                  </td>
                  <td class="agenda-events">
                    <div class="agenda-event">
                      <i class="fa fa-chevron-right fa-fw"></i> Concluding Remarks
                    </div>
                  </td>
                </tr>
              </tbody>

            </table>
					</article>
                
                    
                <div class="wrapper style2" id="papers">
					<article class="container special">
						<header>
                            <h2><a href="#">Accepted Presentations</a></h2>
						</header>
                        The following papers will be presented during the workshop:
                        <table style="width:100%;margin-left:auto;margin-right:auto;">
                        <tr style="border-top:2px solid;border-bottom: 1px solid;">
                            <td>Yongqin Xian and Bernt Schiele and Zeynep Akata
                            </td>
                            <td>"Zero-Shot Learning - The Good, the Bad and the Ugly"
                                <a href="https://drive.google.com/open?id=0BwO69G0kg_oCeTR3TW9FTzNBdG5wN3RZOG5ZRVBPUm5YLVF3">abstract</a>
                           </td>
                            <td><b>Teaser-S1, Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Sujoy Paul, Jawadul H. Bappy, and Amit Roy-Chowdhury</td>
                            <td>"Non-Uniform Subset Selection for Active Learning in Structured Data"
                                <a href="http://www.ee.ucr.edu/~mbappy/pubs/cvpr2017subset.pdf">full paper</a>
                            </td>
                            <td><b>Teaser-S1, Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Yu-Xiong Wang and Deva Ramanan and Martial Hebert</td>
                            <td>"Growing a Brain: Fine-tuning by Increasing Model Capacity"
                                <a href="http://ri.cmu.edu/wp-content/uploads/2017/06/yuxiongw_cvpr17_growingcnn.pdf">full paper</a>
                            </td>
                            <td><b>Teaser-S1, Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Rahaf Aljundi and Tinne Tuytelaars</td>
                            <td>"Continual learning with hebbian synapses"
                                <a href="https://drive.google.com/open?id=0BwO69G0kg_oCeS1vWlR3cnJTOWVIUDlqYjBIaFYyZFYxVEZn">abstract</a>
                            </td>
                            <td><b>Teaser-S1, Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Kai Chen and Hang Song and Chen Change Loy and Dahua Lin</td>
                            <td>"Discover and Learn New Objects from Documentaries"
                                <a href="http://www.chenkai.site/projects/documentary-learning/index.html">project page</a>   
                            </td>
                            <td><b>Teaser-S2, Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Manikandasriram S.R. and Cyrus Anderson and Ram Vasudevan and Matthew Johnson-Roberson</td>
                            <td>
                                "Failing to learn: Autonomously Identifying Perception Failures for Self-driving Cars"
                                <a href="https://arxiv.org/abs/1707.00051">full paper</a>
                            
                            </td>
                            <td><b>Teaser-S2, Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Yizhou Yu and Weifeng Ge</td>
                            <td>"Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-Tuning"
                                <a href="http://i.cs.hku.hk/~yzyu/publication/SelectiveJoint-cvpr17.pdf">full paper</a>
                            </td>
                            <td><b>Teaser-S2, Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;border-top:1px solid;">
                            <td>Maggie Wigness and Douglas Summers-Stay</td>
                            <td>"Continuous and Open-Set Learning for Intelligent Systems Used in Military Relevant Environments"
                                <a href="https://drive.google.com/open?id=0BwO69G0kg_oCN2V4X0lLbWh3T29CX1hkbjA4dFl6eEVQZmU0">abstract</a>
                            </td>
                            <td><b>Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Jason Owens and Philip Osteen</td>
                            <td>"Adaptive Perception Processes for Learning from Experience"
                                <a href="https://drive.google.com/open?id=0BwO69G0kg_oCQWxLVVNmd1F6eFQ2ZnN4UENBNDg1RmxHc3M4">abstract</a>
                            </td>
                            <td><b>Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Judy Hoffman and Dequan Wang and Fisher Yu and Trevor Darrell</td>
                            <td>"FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation"
                                <a href="https://arxiv.org/abs/1612.02649">full paper</a>
                            </td>
                            <td><b>Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Jiseob Kim and Byoung-Tak Zhang</td>
                            <td>"Talking to Teach a Personal Service Robot to Get Acquainted with the Dynamically Changing Environment"
                                <a href="https://drive.google.com/open?id=0BwO69G0kg_oCTDRlMm0wRTR2bEx4ZEdGbVU2ZS1fMGExQUw">abstract</a>
                            </td>
                            <td><b>Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Xin Li and Fuxin Li</td>
                            <td>"Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics"
                                <a href="https://drive.google.com/open?id=0BwO69G0kg_oCMVNKTDR4bHhMM2JZT1ZQb2NQYnh4LWtmMnJj">abstract</a>
                            </td>
                            <td><b>Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>David Krueger and Tegan Maharaj</td>
                            <td>"Reserve output units for deep open-set learning"
                                <a href="https://drive.google.com/open?id=0BwO69G0kg_oCWktSRHBuWmlKQUdQMHJVcHE1RlB0VHlCdUV3">abstract</a>
                            </td>
                            <td><b>Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Jonas Jaeger and Viviane Wolff and Klaus Fricke-Neuderth and Gereon Reus  and Joachim Denzler</td>
                            <td>"A Generic Architecture for Lifelong Learning Portals"
                                <a href="https://drive.google.com/open?id=0BwO69G0kg_oCcEpmT2h4WkNWY2JTeE1MZThSY0xwWmRWU1Vj">abstract</a>
                            </td>
                            <td><b>Poster</b></td>
                        </tr>
                        <tr style="border-bottom:1px solid;">
                            <td>Clemens-Alexander Brust and Christoph Kaeding and Joachim Denzler</td>
                            <td>"Active Learning for Deep Object Detection"
                                <a href="https://drive.google.com/open?id=0BwO69G0kg_oCX2dYenNnU1NsOGc">abstract</a>
                            </td>
                            <td><b>Poster</b></td>
                        </tr>
                        <tr style="border-bottom:2px solid;">
                            <td>Ethan M. Rudd and Lalit P. Jain and Walter J. Scheirer and Terrance E. Boult</td>
                            <td>"The Extreme Value Machine"
                                <a href="https://arxiv.org/pdf/1506.06112.pdf">full paper</a>
                            </td>
                            <td><b>Poster</b></td>
                        </tr>

						</table>
					</article>

				</div>

                    
            <!-- Main -->
				<div class="wrapper style2" id="organizers">
					<article class="container special">
						<header>
							<h2><a href="#">Organizers</a></h2>
						</header>
                        <ul style="margin-left:15%;">
                            <li>
                            <a href="http://www.erodner.de" target="new">
                            <img src="zeisslogo.png" style="max-width:10%;vertical-align:middle;margin-right:5%;" />
                            Erik Rodner (Zeiss Corporate Research and Lecturer at University of Jena, Germany)
                            </a>
							              </li>
                        </ul>
                        <ul style="margin-left:15%;">
                            <li>
                            <a href="http://www.zeiss.de" target="new">
                            <img src="zeisslogo.png" style="max-width:10%;vertical-align:middle;margin-right:5%;" />
                            Alexander Freytag (Zeiss Corporate Research, Germany)
                            </a>
							              </li>
                        </ul>
                        <ul style="margin-left:15%;">
                            <li>
                            <a href="http://www.pub.ist.ac.at/~chl/" target="new">
                            <img src="istlogo.png" style="max-width:10%;vertical-align:middle;margin-right:5%;" />
                            Christoph Lampert (IST Austria)
                            </a>
							              </li>
                        </ul>
                        <ul style="margin-left:15%;">
                            <li>
                            <a href="http://www.vast.uccs.edu/~tboult/index.html" target="new">
                            <img src="coloradologo.png" style="max-width:10%;vertical-align:middle;margin-right:5%;" />
                            Terrance E. Boult (University of Colorado, Colorado Springs, USA)
                            </a>
							              </li>
                        </ul>
                        <ul style="margin-left:15%;">
                            <li>
                            <a href="http://www.inf-cv.uni-jena.de/denzler" target="new">
                            <img src="jenalogo.png" style="max-width:10%;vertical-align:middle;margin-right:5%;" />
                            Joachim Denzler (Computer Vision Group, University of Jena, Germany)
                            </a>
							              </li>
                        </ul>
					</article>
				</div>


				</div>
			<!-- Banner -->
            <section id="banner">
            </section>
				<div class="wrapper style2" id="cfp">
					<article class="container special">
					<header>
                        <h2><a href="#">Call for Abstracts and Participation</a></h2>
				    </header>
                    <p>
                    Recent breakthroughs in our community have relied on the availability of large representative datasets for training. However, the implicit assumption imposed in the majority of our today’s techniques is a static closed world, i.e., non-varying distributions for a fixed set of categories and tasks. Intuitively, these assumptions rarely hold in many application areas such as concept detection in biomedical image analysis, explorative data-driven science, scene parsing for autonomous driving, or household robotics. Instead, the set of semantic concepts and relevant tasks is dynamically changing - even on a daily basis. The assumption of a closed and static world is therefore one of the major obstacles when building intelligent systems that learn continuously, adaptively, and actively.
                    </p>

                    <p>
                    In general, this workshop tries to bridge one of the gaps between computer vision research and AI goals by focusing on different aspects of continuous and open-set learning. In consequence, the following topics will be central to the workshop:
                    <ul style="list-style-type:disc;margin-left:15%">
                      <li>Dealing with partially unknown, open, or dynamically increasing label spaces (probabilistic models, possibility for rejection, novelty detection, etc.)</li>
                      <li>Continuous, online, and incremental learning (at level of instances, classes, common-sense knowledge, and representations)</li>
                      <li>Active acquisition and annotation of new data with humans in the loop (curriculum learning, active learning, etc.)</li>
                      <li>Transfer learning and domain adaptation in continuous and open-set learning scenarios</li>
                      <li>Active data discovery in explorative data science and large-scale microscopy data</li>
                      <li>Benchmarking success in continuous and open-set learning scenarios</li>
                     </ul>
                    </p>
                    <p style="font-size: 130%; text-align: center;">
                    <b>
                        We invite abstract submissions of 1-page in general following the <a href="http://cvpr2017.thecvf.com/submission/main_conference/author_guidelines">CVPR17 format</a>.<br> Submission site: <a href="https://goo.gl/forms/m8OWAp3clVCPc6uF2">https://goo.gl/forms/m8OWAp3clVCPc6uF2</a>
                    <br>
                        Deadline: 31st of May 2017
                    </b>
                    </p><p>
                        The abstract will <b>not appear</b> in any proceedings and if accepted only appear online on this page (if authors like).
                        Our workshop is not meant as a publication venue, but rather a real meeting, where you learn about people interested in the same area and find the next cooperation partners for your future project.
                    </p><p>
                       Accepted abstracts will be presented in a quick <b>talk and a poster</b>. We also welcome submissions of industrial
                       partners interested in the topic and willing to present their application area. Furthermore, if you want to present your next proposal idea and you are looking for cooperation partners, you are also very much invited to submit an abstract.  
                    </p>
                    </article>
				</div>

			<!-- Main -->
<!--
				<div class="wrapper style2" id="program">
					<article class="container special">
						<header>
							<h2><a href="#">Program</a></h2>
						</header>
                        <table style="width:70%;margin-left:auto;margin-right:auto;">
                        <tr style="border-top:1px solid;">
						<td>9:00 - 9:05</td>
                        <td>Workshop Opening</td>
                        </tr> 

                        <tr>
                        <td>9:05 - 9:45</td>
                        <td>Talk of <a href="http://www.uwe-franke.de/" target="new">Uwe Franke</a> (Daimler AG, Germany)</td>
                        </tr>                        
                        <tr>
                        <td></td>
                        <td>"Vision, you can drive my car"</td>
                        </tr>

                        <tr>
						<td>9:45 - 10:30</td>
                        <td>Talk of <a href="http://www.eecs.berkeley.edu/~trevor/" target="new">Trevor Darrell</a> (EECS, UC Berkeley)</td>
                        </tr>                        
                        <tr>
                        <td></td>
                        <td>"The Berkeley DeepDrive Initiative"</td>
                        </tr>										

                        <tr style="border-bottom:1px solid;border-top:1px solid">
						<td>10:30 - 11:00</td>
                        <td>Coffee Break</td>
                        </tr>                        

                        <tr>
						<td>11:00 - 11:45</td>
                        <td>Talk of <a href="https://en.wikipedia.org/wiki/Roger_Melen" target="new">Roger D. Melen</a> (Toyota ITC, USA)</td>
                        </tr>
                        <tr>
                        <td></td>
                        <td>"Considerations For Future Automated and Autonomous Vehicle Designs"</td>
                        </tr>

                        <tr>
                        <td>11:45 - 12:30</td>
                        <td>Poster Session (accepted abstracts)</td>
                        </tr>

                        <tr style='border-bottom: 1px solid;border-top: 1px solid;'>
                        <td>12:30 - 14:00</td>
                        <td>Lunch Break</td>
                        </tr>
						<tr>

                        <tr>
                        <td>14:00 - 15:00</td>
                        <td>Talk of <a href="http://www.cs.toronto.edu/~urtasun/" target="new">Raquel Urtasun</a> (University of Toronto)</td>
                        </tr>
                        <tr>
                        <td></td>
                        <td>"Towards affordable self-driving cars"</td>
                        </tr>

                        <tr style="border-bottom:1px solid;border-top:1px solid">
                        <td>15:00 - 15:45</td>
                        <td>Coffee Break</td>
                        </tr>

                        <tr>
                        <td>15:45 - 16:45</td>
                        <td><a href="http://caffe.berkeleyvision.org" target="new">Caffe</a> Tutorial given by <a href="http://imaginarynumber.net/" target="new">Evan Shelhamer</a> (UC Berkeley)</td>
                        </tr>

                        <tr>
                        <td>16:45 - 17:30</td>
                        <td>Panel session</td>
                        </tr>

                        <tr style="border-bottom:1px solid;">
                        <td>17:30 - </td>
                        <td>Workshop closing</td>
                        </tr>

                        </table>
					</article>

				</div>
-->

			<!-- Footer -->
				<div id="footer">
					<div class="container">
						<div class="row">
							<div class="12u">

								<!-- Copyright -->
									<div class="copyright">
										<ul class="menu">
											<li>&copy; Erik Rodner. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
										</ul>
									</div>

							</div>

						</div>
					</div>
				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.onvisible.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
